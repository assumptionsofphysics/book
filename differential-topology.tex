\chapter{Differential geometry}

\section{Differentiability}

The main idea is to just use the vector space structure of $\mathbb{R}^n$ to define a notion of differential, and then recover differentiability as maps that preserve that notion.

\subsubsection{Differential}

We define the differential as a sequence that converges to zero along a specific direction.

\begin{defn}[Convergence envelope]
	A \textbf{convergence envelope} is a sequence of non-zero elements of $\mathbb{R}$ that converges to $0$.
\end{defn}

\iffalse

\begin{defn}[Convergence at the same rate]
	Let $\{a_i\}_{i=1}^{\infty}$ and $\{b_i\}_{i=1}^{\infty}$ be two convergence envelopes. We say that they \textbf{converge at the same rate} if 
	$$ \lim\limits_{i \to \infty} \frac{a_i}{b_i} = 1.$$
\end{defn}

\begin{prop}
	Convergence at the same rate is an equivalence relation.
\end{prop}

\begin{proof}
	For reflexivity:
	$$ \lim\limits_{i \to \infty} \frac{a_i}{a_i} = \lim\limits_{i \to \infty} 1 = 1.$$
	For symmetry:
	$$ \lim\limits_{i \to \infty} \frac{b_i}{a_i} = \lim\limits_{i \to \infty} \frac{1}{\frac{a_i}{b_i}} = \frac{ \lim\limits_{i \to \infty} 1}{\lim\limits_{i \to \infty} \frac{a_i}{b_i}} = \frac{1}{1} = 1.$$
	For transitivity:
	$$ \lim\limits_{i \to \infty} \frac{a_i}{c_i} = \lim\limits_{i \to \infty} \frac{a_i}{b_i}\frac{b_i}{c_i} = \lim\limits_{i \to \infty} \frac{a_i}{b_i} \lim\limits_{i \to \infty} \frac{b_i}{c_i} = 1 \cdot 1 = 1.$$
\end{proof}

\begin{defn}[Convergence class]
	A \textbf{convergence class} is an equivalence class of sequences that converge at the same rate.
\end{defn}
\fi

\begin{defn}
	Let $V$ be a real vector space. A \textbf{differential} $dv$ is a sequence of vectors $\{v_i\}_{i=1}^{\infty}$ such that there exists a vector $t \in V$ and a convergence envelope $\{a_i\}_{i=1}^{\infty}$ such that 
	$$ \lim\limits_{i \to \infty} \frac{v_i}{a_i} = t.$$
	We call $t$ the \textbf{tangent vector} of the differential and $\{a_i\}_{i=1}^{\infty}$ its \textbf{convergence envelope}. We note $dv[a_i \, t]$ the differential with its tangent vector and convergence envelope.
\end{defn}

\begin{prop}
	Let $dv$ be a differential. It can be expressed as
	$$ v_i = a_i t_i = a_i(t + w_i)$$
	where $\{t_i\}_{i=1}^{\infty}$ is a sequence of vectors that converges to $t$ and $w_i$ is a sequence of vectors such that converges to $0$.
\end{prop}

\begin{proof}
For the first expression, we can write $t_i = \frac{v_i}/{a_i}$. Given the definition of differential, that sequence converges to $t$.	
	
For the second expression, we can write $v_i = a_i v + a_i \left( \frac{v_i}{a_i} - v\right)$ for all $i$. We can set $w_i = \frac{v_i}{a_i} - v$ and write $v_i = a_i(v + w_i)$. We have:
$$\lim\limits_{i \to \infty} \frac{v_i}{a_i} = \lim\limits_{i \to \infty} \frac{a_i v}{a_i} + \lim\limits_{i \to \infty} \frac{a_i w_i}{a_i} = \lim\limits_{i \to \infty} v + \lim\limits_{i \to \infty} w_i$$
The sequence on the left converges to $v$, the first sequence on the right converges to $v$, therefore the second sequence on the right converges to zero.
\end{proof}

\begin{prop}
	Differentials respect the following property
	$$ dv[a_i \, kt] = dv[k a_i \, t].$$
	That is, a differential with tangent vector $kt$ and convergence envelope $\{a_i\}_{i=1}^{\infty}$ is also a differential with tangent vector $t$ and convergence envelope $\{ka_i\}_{i=1}^{\infty}$.
\end{prop}

\begin{proof}
	We have $v_i = a_i (k t_i) = (k a_i) t_i$.
\end{proof}

\iffalse

\begin{prop}
	Let $\{v_i\}_{i=1}^{\infty}$ be a directionally convergent sequence with tangent vector $v$ and rate of convergence $\{a_i\}_{i=1}^{\infty}$. Let $\{b_i\}_{i=1}^{\infty}$ be a convergence envelope that converges at the same rate of $\{a_i\}_{i=1}^{\infty}$. Then $\{v_i\}_{i=1}^{\infty}$ is also a directionally convergent sequence with tangent vector $v$ and rate of convergence $\{b_i\}_{i=1}^{\infty}$.
\end{prop}

\begin{proof}
	Note that 
	$$ \lim\limits_{i \to \infty} \frac{v_i}{b_i} = \lim\limits_{i \to \infty} \frac{v_i}{a_i}\frac{a_i}{b_i} = \lim\limits_{i \to \infty} \frac{v_i}{a_i} \lim\limits_{i \to \infty} \frac{a_i}{b_i} = \lim\limits_{i \to \infty} \frac{v_i}{a_i}.$$
	Therefore all convergence envelopes that converge at the same rate will yield the same results.
\end{proof}

\fi

\subsubsection{Differentiability}

We define a map to be differentiable if it maps differentials to differentials with the same convergence envelope.

\begin{defn}
	Let $V$ and $W$ be two vector spaces and let $v_0 \in V$. Given a map $f: V \to W$, the \textbf{differential} of $f$ is the map $df(v_i, dv)$ that given a convergent sequence $\{v_i\}_{i=1}^{\infty}$ and a differential $dv$ returns the sequence $\{f(v_i + a_i t_i) - f(v_i)\}_{i=1}^{\infty}$. The map is \textbf{differentiable} at $v$ if there exists a map $\left.\frac{df}{dV} \right|_{v}: V \to W$, called \textbf{derivative} such that $df(v_i, dv[a_i \, t]) = dw[a_i \, \left.\frac{df}{dV} \right|_{v} (t)]$. That is, $df$ maps differentials of $V$ to differentials of $W$ that have the same convergence envelope and a tangent vector that depends only on the original tangent vector.
\end{defn}

\begin{prop}
	A map $f : V \to W$ is differentiable at $v$ if and only if the limit
	$$ \lim\limits_{i \to \infty} \frac{f(v_i + a_i t_i) - f(v_i)}{a_i} = \left.\frac{df}{dV} \right|_{v} (t)$$
	converges for sequences $\{v_i\}_{i=1}^{\infty}$ that converge to $v$, $\{a_i\}_{i=1}^{\infty}$ that converge to $0$ with $a_i \neq 0$ for all $i$ and $\{t_i\}_{i=1}^{\infty}$ that converges to some $t$.
\end{prop}

\begin{proof}
	The expression is simply applying the definition of differential to $df$.
\end{proof}

\begin{remark}
	This definition of differentiability implies Hadamard differentiability and Gateaux differentiability. This means that, for normed vector spaces, it also implies Frechet differentiability. Unlike Hadamard and Gateaux, it implies linearity, which therefore is not an extra condition.
\end{remark}

\begin{prop}
	The derivative must be a linear function.
\end{prop}

\begin{proof}
	Recall that $dv[a_i \, kt] = dv[k a_i \, t]$, therefore $dw[a_i \, \left.\frac{df}{dV} \right|_{v} (kt)] = df(v_i, dv[a_i \, kt]) = df(v_i, dv[ka_i \, t]) = dw[ka_i \, \left.\frac{df}{dV} \right|_{v} (t)] = dw[a_i \, k \left.\frac{df}{dV} \right|_{v} (t)]$. Therefore $\left.\frac{df}{dV} \right|_{v} (kt) = k \left.\frac{df}{dV} \right|_{v} (t)$.
	
	We also have $dw[a_i \, \left.\frac{df}{dV} \right|_{v} (t+u)] = df(v_i, dv[a_i \, t + u]) = \{f(v_i + a_i (t_i + u_i)) - f(v_i)\}_{i=1}^{\infty} = \{f(v_i + a_i (t_i + u_i)) - f(v_i + a_i t_i) + f(v_i + a_i t_i) - f(v_i)\}_{i=1}^{\infty} = \{f((v_i + a_i t_i) + a_i u_i) - f(v_i + a_i t_i)\}_{i=1}^{\infty} + \{f(v_i + a_i t_i) - f(v_i)\}_{i=1}^{\infty} = df(v_i + a_i t_i, dv[a_i \, u]) + df(v_i, dv[a_i \, t]) = dw[a_i \, \left.\frac{df}{dV} \right|_{v} (u)]+dw[a_i \, \left.\frac{df}{dV} \right|_{v} (v)] = dw[a_i \, \left.\frac{df}{dV} \right|_{v} (u) + \left.\frac{df}{dV} \right|_{v} (v)]$.
\end{proof}

\begin{prop}
	Let $f:\mathbb{R} \to \mathbb{R}$. Then the standard analytical notions of differentiability and derivative coincide.
\end{prop}
\begin{proof}
	The standard notion of differentiability at $v \in \mathbb{R}$ requires the limit
	$$ \lim\limits_{h \to 0} \frac{f(v + h) - f(v)}{h} $$
	to exist. This is equivalent to requiring that the limit
	$$ \lim\limits_{i \to \infty} \frac{f(v + h_i) - f(v)}{h_i} $$
	exist exists for all sequences $\{h_i\}_{i=1}^{\infty}$ that converge to zero with $h_i \neq 0$ for all $i$.
	
	Our notion of differentiability at $v \in \mathbb{R}$ requires the limit
	$$ \lim\limits_{i \to \infty} \frac{f(v_i + a_i t_i) - f(v_i)}{a_i} = \left.\frac{df}{dV} \right|_{v} (t)$$
	to exist for all sequences $\{v_i\}_{i=1}^{\infty}$ that converge to $v$, $\{a_i\}_{i=1}^{\infty}$ that converge to $0$ with $a_i \neq 0$ for all $i$, and $\{t_i\}_{i=1}^{\infty}$ that converges to some $t$.
	
	Suppose $f$ is differentiable in the new sense. Choose $t_i = 1$ and $v_i = v$ for all $i$ we have
	$$ \lim\limits_{i \to \infty} \frac{f(v + a_i) - f(v)}{a_i} = \left.\frac{df}{dV} \right|_{v} (1)$$
	for all $\{a_i\}_{i=1}^{\infty}$ that converge to $0$ with $a_i \neq 0$ for all $i$. Therefore $f$ is differential in the standard sense.
	
	Suppose $f$ is differentiable in the standard sense. Assuming, for now, that all sequences that appear at the denominator are never zero, we have
	\begin{align*}
	\lim\limits_{i \to \infty} \frac{f(v_i + a_i t_i) - f(v_i)}{a_i} &= \lim\limits_{i \to \infty} \frac{f(v + (v_i - v) + a_i t_i) - f(v + (v_i - v))}{a_i} \\
	&= \lim\limits_{i \to \infty} \frac{f(v + (v_i - v) + a_i t_i) - f(v) + f(v) - f(v + (v_i - v))}{a_i} \\
	&= \lim\limits_{i \to \infty} \left[\frac{f(v + (v_i - v) + a_i t_i) - f(v)}{a_i}
	- \frac{f(v + (v_i - v)) - f(v)}{a_i} \right]\\
	&= \lim\limits_{i \to \infty} \left[ \frac{f(v + (v_i - v) + a_i t_i) - f(v)}{(v_i - v) + a_i t_i} \frac{(v_i - v) + a_i t_i}{a_i} \right. \\
	&- \left.\frac{f(v + (v_i - v)) - f(v)}{(v_i - v)}  \frac{(v_i - v)}{a_i} \right] \\
	&=  \left.\frac{df}{dv} \right|_{v} \lim\limits_{i \to \infty} \left[ \frac{(v_i - v) + a_i t_i}{a_i} -  \frac{(v_i - v)}{a_i}\right] = \left.\frac{df}{dv} \right|_{v} \lim\limits_{i \to \infty} t_i = \left.\frac{df}{dv} \right|_{v} t \\
	\end{align*}
\end{proof}

\begin{remark}
	Let us calculate the derivative of $x^2$ at a generic point $x$ with a generic differential $dx = \{a_i t_i\}$.
	$$ \lim\limits_{i \to \infty} \frac{(x_i + a_i t_i)^2 - x_i^2}{a_i} = \lim\limits_{i \to \infty} \frac{x_i^2 + 2 x_i a_i t_i + a_i^2t_i^2 - x_i^2}{a_i} = \lim\limits_{i \to \infty} \frac{2 x_i a_i t_i + a_i^2t_i^2}{a_i}$$
	$$= \lim\limits_{i \to \infty} 2 x_i t_i + a_i t_i^2=2xt$$
	This shows that higher orders here are simply higher powers of $a_i$.
\end{remark}

\begin{remark}
	Consider a sequence $v_i$. Suppose it converges to $v$. We can write the differential $v_i - v$. Now suppose it is a differential. Then we can find $a_i$ such that $\frac{v_i - v}{a_i} = t_i$ converge to $t$. We can write $t_i - t$. Suppose this is also a differential with the same convergence envelope. Then $\frac{t_i - t}{a_i} = c_i$ converges to some $c$.
	
	Putting all together, we can write $v_i = a_i^0 v + a_i^1 t + a_i^2 c + a_i^3 w_i$ where $w_i = c_i - c$. $v$, $t$ and $c$ represent repsectively the zeroth, first and second order term of the sequence. $w_i$ is the higher order.
	
	Suppose the index $i$ does not span to infinity, but ends at some finite $n$. Then the $n$ sequences $a_i^j$ with $0 \leq k < n$ are linearly independent. Then we can write the finite sequence $v_i$ as $v_i = \sum c_j a_i^j$. If we take the limit as $n$ goes to infinity, this does not work: all the sequences $\{a_i^j\}_{i=0}^\infty$ converge for any $j$, therefore we can only write sequences that converge. An infinitely smooth sequence, then, is one that can be expressed with the infinite sum. An infinitely smooth function retains, in a sense, the property that it can be decomposed like a finite sequence.
\end{remark}


\begin{prop}[Chain rule]
	Let $U$, $V$ and $W$ be three vector spaces. Let $f : U \to V$ and $g : V \to W$ be two differentiable maps and $h = g \circ f$ their composition. Then $\frac{dh}{dU} = \frac{dg}{dV} \circ \frac{df}{dU}$.
\end{prop}

\begin{proof}
	Given that $f$ and $g$ are differentiable, a differential of $U$ will be mapped to a differential of $V$ which will be mapped to a differential of $W$. The composite map $h$ will map a differential of $U$ to a differential of $W$ and therefore it is differentiable and the derivative exists. The differential returned by $h$ will be the one returned by $g$ and $f$ combined, therefore the derivative of $h$ is the function combination of the derivatives of $g$ and $f$.
\end{proof}

\begin{remark}
	The chain rule in this framework is just function combination. Given that these are linear function, it reduces to multiplication of the derivative on $\mathbb{R}$ and on matrix multipliciation on $\mathbb{R}^N$.
\end{remark}


\subsubsection{Partial derivatives}

TODO: still old version

Partial derivatives are recovered studying how differentials behave under vector space composition. If $f : V \to W$ is differentiable and $V = V_1 \times V_2$, we can ask for the map from a differential of $V_1$ to a differential of $W$ through $f$. This is exactly a variation of $V$ that keeps $V_2$ constant. Conceptually, given that $dv = (dv_1, dv_2)$ and the derivative is linear, we have $dw = \frac{df}{dV} dv = \frac{df}{dV} (dv_1, dv_2) = \frac{df}{dV} (dv_1, 0) + \frac{df}{dV} (0, dv_2) = \frac{df}{dV_1} dv_1 + \frac{df}{dV_2} dv_2$.
	
Suppose that $V = V_1 \times V_2$, then $dV = dV_1 \times dV_2$. This means that we can decompose a differential $dv= dv_1 + dv_2$ as the sum of two differentials in the respective spaces (mapped into the composite through the injection map). Suppose that $W = W_1 \times W_2$ and $f : V \to W$ is differentiable. Then we also have $dw= dw_1 + dw_2$ and we can study of $f$ maps differentials of $V_1$ and $V_2$ to differentials of $W_1$ and $W_2$. We will find $dw = dw_1 + dw_2 = \frac{dw_1}{dV} dv + \frac{dw_2}{dV} dv = \frac{\partial w_1}{\partial V_1} dv_1 + \frac{\partial w_1}{\partial V_2} dv_2 + \frac{\partial w_2}{\partial V_1} dv_1 + \frac{\partial w_2}{\partial V_2} dv_2$. If $U = \mathbb{R}^n$ and $V = \mathbb{R}^m$ we find the usual definitions.

\begin{prop}[Direct product and direct sum]
	The differential space $dV$ of the direct product $V=\prod V_i$ of a family of vector spaces $V_i$ is the direct product $\prod dV_i$ of the respective differential spaces. The same is true for the direct sum.
\end{prop}

\begin{proof}
	Let $v \in V$ and consider its halving sequence $\{(1/2)^i v\}_{i=1}^{\infty}$. Given that $V$ is a direct product, $v=(v_j)$ we have $\{(1/2)^i (v_j)\}_{i=1}^{\infty} = \{((1/2)^i v_j)\}_{i=1}^{\infty}$. Conversely, if we pick one vector for each space $V_j$ and construct their respective halving sequences, this becomes a halving sequence of a vector $v \in V$. Therefore the space of halving sequences over $V$ is the direct product of the halving sequences over the respective $V_j$. Given that the spaces of halving sequences are isomorphic to the respective differential spaces, the differential space of the direct product is the direct product of the differential spaces.
	
	The same argument works for direct sum.
\end{proof}

\begin{defn}
	Let $V$ and $W$ be two vector spaces and let $V=\prod V_i$ be the direct product of a family of vector spaces $V_i$. Let $f: V \to W$ be a differentiable map. The \textbf{partial derivative} of $f$ with respect to $W_i$, noted $\frac{\partial f}{\partial V_i}$, is the map from a differential of $V_i$ to the differential of $W$ through $f$. That is, $\frac{\partial f}{\partial V_i} (dv_i) = \frac{\partial f}{\partial V} \left( (0, ..., 0, dv_i, 0, ...) \right)$.
\end{defn}

\begin{prop}
	Let $f:\mathbb{R}^m \to \mathbb{R}^n$. Then the standard analytical notions of differentiability and partial derivatives coincide with the new ones.
\end{prop}

\begin{proof}
\end{proof}


\begin{remark}
	Partial derivatives should be recoverable through vector space composition. Suppose that $V = V_1 \times V_2$, then $dV = dV_1 \times V_2$. This means that we can decompose a differential $dv= dv_1 + dv_2$ as the sum of two differentials in the respective spaces (mapped into the composite through the injection map). Suppose that $W = W_1 \times W_2$ and $f : V \to W$ is differentiable. Then we also have $dw= dw_1 + dw_2$ and we can study of $f$ maps differentials of $V_1$ and $V_2$ to differentials of $W_1$ and $W_2$. We will find $dw = dw_1 + dw_2 = \frac{dw_1}{dV} dv + \frac{dw_2}{dV} dv = \frac{\partial w_1}{\partial V_1} dv_1 + \frac{\partial w_1}{\partial V_2} dv_2 + \frac{\partial w_2}{\partial V_1} dv_1 + \frac{\partial w_2}{\partial V_2} dv_2$. If $U = \mathbb{R}^n$ and $V = \mathbb{R}^m$ we find the usual definitions.
\end{remark}


\section{Linear functionals}

Review notation. Let $M$ be a differentiable manifold of dimension $n$.

\begin{defn}
	A $k$-surface is a $k$-dimensional smooth submanifold of $M$. We denote by $S^k$ the set of all  $k$-surfaces of dimension $k$ and by $S = \bigcup_{k=0}^n S^k$ the set of all smooth surfaces of all dimensions.
\end{defn}

\begin{defn}
	Given a $k$-surface $\sigma^k \in S^k$, the \textbf{boundary} of $\sigma^k$, denoted by $\partial\sigma^k \in S^{k-1}$ is the limit of varied coordinates. The \textbf{boundary operator} $\partial : S \to S$ is a map from a $k$-surface to its boundary. A surface is \textbf{closed} if has no boundary.
\end{defn}

\begin{coro}
	Boundary of smooth surfaces are smooth surfaces. Boundaries do not have boundaries. That is, $\partial\partial \sigma^k = \emptyset$ for all $\sigma^k \in S^k$.
\end{coro}


\begin{defn}
	A \textbf{$k$-functional} is a linear function of $k$-surfaces. That is, it is a function $f_k : S^k \to \mathbb{R}$ with the following properties:
	\begin{description}
		\item[Linear] $f_k(\sigma^k_1 \cup \sigma^k_2) = f_k(\sigma^k_1) + f_k(\sigma^k_2)$ for every $\sigma^k_1, \sigma^k_2 \in S^k$ such that $\sigma^k_1 \cap \sigma^k_2 = \emptyset$
		\item[No contribution from boundary] $f_k(\sigma^k) = f_k(\sigma^k \setminus \partial \sigma^k)$ for every $\sigma^k \in S^k$
		\item[Commutes with the limit] $\lim\limits_{i \to \infty} f_k(\sigma_i^k) = f_k(\lim\limits_{i \to \infty}\sigma_i^k)$
	\end{description}
	We denote by $F_k$ the set of all $k$-functionals of dimension $k$ and by $F = \bigcup_{k=0}^nF_k$ is the set of all $k$-functionals.
\end{defn}

\begin{coro}
	Any $k$-functional applied to the empty set returns zero. That is, for any $f_k \in F$, $f_k(\emptyset) = 0$.
\end{coro}

\begin{defn}
	The \textbf{zero $k$-functional}, noted $0_k \in F_k$, is the $k$-functional that always returns zero. That is, $0_k(\sigma^k) = 0$ for all $\sigma^k \in S^k$.
\end{defn}


\begin{defn}
	Given a $k$-functional $f_k \in F_k$, the \textbf{boundary functional} $\partial f_k \in F_{k+1}$ is a $(k+1)$-functional that applies $f_k$ on the boundary. That is, $\partial f_k(\sigma^{k+1}) = f_k(\partial \sigma^{k+1})$. 
\end{defn}

\begin{coro}
	The boundary functional of the boundary functional is the zero functional. That is, for any $k$-functional $f_k \in F$, $\partial \partial f_k = 0_{k+2}$.
\end{coro}

\begin{proof}
	$\partial \partial f_k (\sigma ^{f+2}) = \partial f_k (\partial \sigma ^{f+2}) = f_k (\partial \partial \sigma ^{f+2}) = f_k(\emptyset) = 0$
\end{proof}

\begin{defn}
	A $k$-surface $\sigma^k \in S^k$ is \textbf{contractible} if it can be continuously shrunk to a point. That is, the inclusion map $\iota : \sigma^k \to X$ is null-homotopic.
\end{defn}

\begin{defn}
	An \textbf{exact functional} is a $k$-functional that returns zero on all closed $k$-surfaces. That is, $f_k(\sigma^k) = 0$ for all $\sigma^k \in S^k$ such that $\partial\sigma^k = \emptyset$. A \textbf{closed functional} returns zero on all contractible closed surfaces.
\end{defn}

\begin{remark}
	Names are chosen to agree with exact/closed forms... Should we find better names?
\end{remark}

\begin{prop}
	Let $f \in F_k$ be an exact $k$-functional. Then there exists some $(k-1)$-functional $g \in F_{k-1}$ such that $f = \partial g$. We say $g$ is the \textbf{potential} of $f$.
\end{prop}

\begin{remark}
	The aim here is to prove the theorem on finite surfaces, without using standard differentiable calculus.
	
	As a model, we should use the standard proof used in physics for irrotational fields. Suppose we have an exact 1-functional, that is it gives zero for all closed lines. Then, one can show that two lines that share the same boundary must have the same value. Then pick a point and assign zero to that point. To any other point, assign the value given by the functional over a line that starts at the zero point and ends that the new point. The potential is given by those assignments.
	
	The way to generalize is to realize that a $k$-surface is half a boundary of a $k+1$-surface. For example, a point is half a boundary of a line, which constitutes of 2 points. The boundary of a surface is a closed line, which can be understood as two lines that share the same boundary, but opposite orientation. Like the line integral can be understood as ``going from`` one point (i.e. half boundary) to the other, the surface integral can be understood as ``going from'' one line (i.e. half boundary) to the other.
	
	For example, suppose we have an exact 2-functional, that is it gives zero for all closed surfaces. Then two surfaces that share the same boundary have the same value. Pick a reference point. Pick a family of lines such that they all start from the reference point, all end at different point (covering the whole space) and never form two paths to the same point. For example, in local coordinates, change one coordinate at a time (i.e. first increase the x, then increase the y, then the z, ...). Now pick a scalar function (a (2 - 2)-form) and assign to each line the difference at the boundary (this arbitrary choice is the equivalent of choosing the constant function in the previous case, and effectively maps to the choice of gauge). Given any other line, we can use the family to find two lines to form a closed loop. A closed loop identifies, which can now be given a value based on the 2-functional.
	
	This should be generalizable with the following sketch. Take a set of surfaces $R \subset S^{k-1}$, called references, that:
	\begin{enumerate}
		\item includes the empty surface $\emptyset$
		\item the union of a family of surface is in $R$
		\item subsurface of a surface is in $R$
		\item no two surfaces share the same boundary.
	\end{enumerate}
	(Note: some care needs to be done with the definition for $k=1$) Therefore, for any $\sigma^{k-1} \in S^{k-1}$ we find a unique $R(\sigma^{k-1}) \in R$ such that $\partial R(\sigma^{k-1}) = \partial \sigma^{k-1}$. That is, for any surface we find a reference surface with the same boundary, and together $\partial \sigma^{k-1} \cup R(\sigma^{k-1})$ they form a closed surface. Since $f$ is exact, $f(\sigma^k)$ depends only on the boundary of $\sigma^k$: two surfaces with equal boundary can be joined together to form a surface with no boundary, for which $f$ is zero. Therefore we can define $\hat{f}_{k-1} : S^{k-1} \to \mathbb{R}$ such that $\hat{f}_{k-1}(\sigma^{k-1}) = f_k(\hat{\sigma}^{k})$ where $\hat{\sigma}^{k}$ is any surface such that $\partial \hat{\sigma}^{k} = \sigma^{k-1}$. Now take an exact functional $v \in F_{k-1}$. Define $g \in F_{k-1}$ such that $g(\sigma^{k-1}) = \hat{f}_{k-1}(\sigma^{k-1} \cup R(\sigma^{k-1})) + v(R(\sigma^{k-1}))$. We have $\partial g(\sigma^{k}) = g(\partial \sigma^{k}) = \hat{f}_{k-1}(\partial \sigma^{k} \cup R(\partial \sigma^{k})) + v(R(\partial \sigma^{k}))$. Since $\partial \partial \sigma^k = \emptyset$, $R(\partial \sigma^{k}) = \emptyset$ because that is the only surface in $R$ with an empty boundary. Therefore $\partial g(\sigma^k) = \hat{f}_{k-1}(\partial \sigma^k \cup \emptyset) + v(\emptyset) = \hat{f}_{k-1}(\partial \sigma^k) = f_k(\sigma^k)$. Which means $\partial g = f$.
\end{remark}


\iffalse

\section{Differential forms}

This section needs to show that vectors and differential forms are infinitesimal counterparts of surfaces and functional. 


\begin{defn}
	TODO: Define a \textbf{$k$-vector} $v^k \in V^k$ as an infinitesimal parallelepiped. We note $V^k$ as the set of all vectors of rank k and $V = \bigcup_{k=0}^n V^k$ as the set of all $k$-vectors. 
\end{defn}


\begin{defn}
	The \textbf{wedge product} $\wedge : V^k\times V^j \to V^{k+j}$ returns the $(k+j)$-vector that represents the parallelopiped formed by the sides represented by the given $k$-vector and $j$-vector. 
\end{defn}


\begin{remark}
	Notation for a generic vector. Infinitesimal displacement $dP$ is a vector and can be expressed as: $dP = dx \frac{\partial P}{\partial x^i}$. We can set $e_i = \frac{\partial P}{\partial x^i}$ so $dP = dx^i e_i$.
	
	Every infinitesimal $k$-surface $d\sigma^k$ can be expressed, in terms of the wedge product, as $d\sigma^k= dx^{i_1}dx^{i_2}...dx^{i_k}\frac{\partial P}{\partial x^1} \wedge \frac{\partial P}{\partial x^2} \wedge ... \wedge \frac{\partial P}{\partial x^k} = dx^{i_1}dx^{i_2}...dx^{i_k} e_1 \wedge e_2 \wedge ... \wedge e_k$.
	
	Suppose we have a $k$-surface in terms of $k$ coordinates $s^j$. We will have a differentiable function $x^i = x^i(s^j)$ that maps the parametrization of the $k$-surface into the manifold. At each point $P$, we can write $dx^i = \frac{\partial x^i}{\partial s^j} ds^j$. Therefore we have $d\sigma^k = dx^{i_1}dx^{i_2}...dx^{i_k} e_1 \wedge e_2 \wedge ... \wedge e_k$.
	
	For example:
	\begin{align*}
		x^i &= \{x,y,z\} \\
		s^j &= \{\varphi, \theta\} \\
		x &= \sin \varphi \cos \theta \\ 
		y &= \sin \varphi \sin \theta \\ 
		z &= \cos \varphi \\ 
		d\sigma &= d\varphi d\theta (e_\varphi \wedge e_\theta) \\
		&=d\varphi d\theta \left(\frac{\partial x}{\partial \varphi} e_x + \frac{\partial y}{\partial \varphi} e_y + \frac{\partial z}{\partial \varphi} e_z\right) \wedge \left(\frac{\partial x}{\partial \theta} e_x + \frac{\partial y}{\partial \theta} e_y + \frac{\partial z}{\partial \theta} e_z\right) \\
		&=d\varphi d\theta (
\frac{\partial x}{\partial \varphi} e_x \wedge \frac{\partial x}{\partial \theta} e_x +
\frac{\partial x}{\partial \varphi} e_x \wedge \frac{\partial y}{\partial \theta} e_y +
\frac{\partial x}{\partial \varphi} e_x \wedge \frac{\partial z}{\partial \theta} e_z + \\
&\frac{\partial y}{\partial \varphi} e_y \wedge \frac{\partial x}{\partial \theta} e_x +
\frac{\partial y}{\partial \varphi} e_y \wedge \frac{\partial y}{\partial \theta} e_y +
\frac{\partial y}{\partial \varphi} e_y \wedge \frac{\partial z}{\partial \theta} e_z + \\
&\frac{\partial z}{\partial \varphi} e_z \wedge \frac{\partial x}{\partial \theta} e_x +
\frac{\partial z}{\partial \varphi} e_z \wedge \frac{\partial y}{\partial \theta} e_y + 
\frac{\partial z}{\partial \varphi} e_z \wedge \frac{\partial z}{\partial \theta} e_z ) \\
		&=d\varphi d\theta ((
\frac{\partial x}{\partial \varphi} \frac{\partial y}{\partial \theta} - \frac{\partial y}{\partial \varphi}\frac{\partial x}{\partial \theta}) e_x \wedge e_y + \\
& (\frac{\partial y}{\partial \varphi} \frac{\partial z}{\partial \theta} - \frac{\partial z}{\partial \varphi} \frac{\partial y}{\partial \theta}) e_y \wedge e_z + \\
&\frac{\partial z}{\partial \varphi} \frac{\partial x}{\partial \theta} - \frac{\partial x}{\partial \varphi} \frac{\partial z}{\partial \theta}) e_z \wedge e_x ) \\
		&=d\varphi d\theta ((
\cos \varphi \cos \theta \sin \varphi \cos \theta - \cos \varphi \sin \theta \sin \varphi (-\sin \theta)) e_x \wedge e_y + \\
& (\cos \varphi \sin \theta  0 - (- \sin \varphi) \sin \varphi \cos \theta) e_y \wedge e_z + \\
& - \sin \varphi \sin \varphi (-\sin \theta) - \cos \varphi \cos \theta 0) e_z \wedge e_x ) \\
		&=d\varphi d\theta (\cos \varphi \sin \varphi e_x \wedge e_y +
\sin^2 \varphi \cos \theta e_y \wedge e_z +
\sin^2 \varphi \sin \theta e_z \wedge e_x )
	\end{align*}
\end{remark}

\begin{defn}
	A \textbf{$k$-form} $\omega_k : V^k \to \mathbb{R}$ is a linear function of a vector. We note $\Omega_k$ as the set of all $k$-forms of dimension k and $\Omega = \cup_{k=0}^n\Omega_k$ as the set of all forms. 
\end{defn}

\begin{prop}
	TODO Show that every k-functional has a corresponding k-form, such that for $f_k = \int_{\sigma^k} \omega_k(d\sigma^k)$. In words, a linear functional applied over a k-surface is the same as an integral of a k-form over the infinitesimal parallelepipedes of that k-surface. 
\end{prop}

\fi




